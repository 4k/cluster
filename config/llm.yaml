# LLM Service Configuration
# ==========================
# This file configures the LLM service connection and model settings.
# Supports Ollama and OpenAI-compatible APIs.
#
# Environment variable overrides use the prefix LLM_
# Example: LLM_BASE_URL=http://192.168.1.100:11434, LLM_DEFAULT_MODEL=llama3.2:3b

# Connection Settings
# -------------------
# Base URL of the LLM server (Ollama instance or OpenAI-compatible API)
base_url: "http://localhost:11434"

# API key for authentication (required for OpenAI, optional for local Ollama)
api_key: null

# API type: auto, ollama-chat, ollama-generate, openai
# "auto" will detect the best endpoint automatically
api_type: "auto"

# Model Settings
# --------------
# Default model to use for requests
# For Ollama: use model names like "llama3.2:3b", "qwen3-coder:30b", etc.
# For OpenAI: use model names like "gpt-4", "gpt-3.5-turbo", etc.
default_model: "llama3.2:3b"

# System prompt for the assistant
# This defines the personality and behavior of the assistant
system_prompt: |
  You are a helpful voice assistant. Provide concise, natural responses
  suitable for voice output. Keep answers brief but informative.

# Request Settings
# ----------------
# Request timeout in seconds
timeout_seconds: 120

# Enable streaming responses
stream: false

# Generation Parameters
# ---------------------
# Temperature (0.0 - 2.0)
# Lower = more focused/deterministic, Higher = more creative/random
temperature: 0.7

# Maximum tokens to generate
max_tokens: 512
